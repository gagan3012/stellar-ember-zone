---
title: 'DateLogicQA: Benchmarking Temporal Biases in Large Language Models'

authors:
  - me
  - Ming Ze Tang
  - Cristina Mahanta
  - Madiha Kazi

date: '2025-04-01T00:00:00Z'
publishDate: '2025-04-01T00:00:00Z'

publication_types: ['paper-conference']

publication: 'In Annual Conference of the North American Chapter of the Association for Computational Linguistics: Student Research Workshop (NAACL SRW 2025)'
publication_short: In *NAACL 2025*

abstract: DateLogicQA introduces a comprehensive benchmark for evaluating temporal biases and reasoning capabilities in Large Language Models. This benchmark systematically tests LLMs on various temporal reasoning tasks to identify failure patterns.

summary: A comprehensive benchmark for evaluating temporal biases and reasoning in Large Language Models.

tags:
  - Large Language Models
  - Temporal Reasoning
  - Benchmarking
  - Evaluation

featured: true

hugoblox:
  ids:
    doi: ''

links:
  - type: pdf
    url: "https://aclanthology.org/2025.naacl-srw.32/"
  - type: source
    url: "https://aclanthology.org/2025.naacl-srw.32/"

image:
  caption: ''
  focal_point: ''
  preview_only: false

projects: []
slides: ""
---

DateLogicQA provides researchers and practitioners with a standardized way to evaluate and compare LLM performance on temporal reasoning tasks, helping identify systematic biases and failure modes.
